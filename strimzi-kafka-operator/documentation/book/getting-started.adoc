== Getting started

=== Downloading {ProductName}

Strimzi releases are available for download from {ReleaseDownload}.
The release artifacts contain documentation and example YAML files for deployment on {ProductPlatformName}.
The example files are used throughout this documentation and can be used to install {ProductName}.
The Docker images are available on {DockerRepository}.

=== Prerequisites

{ProductLongName} runs on {ProductPlatformName}.
{ProductName} supports
ifdef::Kubernetes[{KubernetesLongName} {KubernetesVersion} or]
{OpenShiftLongName} {OpenShiftVersion}.
{ProductName} works on all kinds of clusters - from public and private clouds down to local deployments intended for development.
This guide expects that an {ProductPlatformName} cluster is available and the
ifdef::Kubernetes[`kubectl` or]
`oc` command line tools are installed and configured to connect to the running cluster.

ifdef::InstallationAppendix[]
When no existing {ProductPlatformName} cluster is available, `Minikube` or `Minishift` can be used to create a local
cluster. More details can be found in <<installing_kubernetes_and_openshift_cluster>>
endif::InstallationAppendix[]

In order to execute the commands in this guide, your {ProductPlatformName} user needs to have the rights to create and
manage RBAC resources (Roles and Role Bindings).

=== Cluster Operator

{ProductName} uses a component called the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect
clusters.
The Cluster Operator is deployed as a process running inside your {ProductPlatformName} cluster.
To deploy a Kafka cluster, a `Kafka` resource with the cluster configuration has to be created within the {ProductPlatformName} cluster.
Based on the information in that `Kafka` resource,
the Cluster Operator will deploy a corresponding Kafka cluster.
The format of the `Kafka` resource is described in <<kafka_assembly_details>>.

{ProductName} contains example YAML files which make deploying a Cluster Operator easier.

ifdef::Kubernetes[]
==== Deploying to {KubernetesName}

To deploy the Cluster Operator on {KubernetesName}, you will first need to modify some of of the installation files according to the namespace the Cluster Operator is going to be installed in. 
To do this, execute the following command, replacing `<my-namespace>` with the correct namespace:

[source,shell]
sed -i 's/namespace: .*/namespace: <my-namespace>/' examples/install/cluster-operator/*ClusterRoleBinding*.yaml

Next, the following commands should be executed:

[source,shell]
kubectl create -f examples/install/cluster-operator

To verify whether the Cluster Operator has been deployed successfully, the {KubernetesName} Dashboard or the following
command can be used:

[source,shell]
kubectl describe all
endif::Kubernetes[]

==== Deploying to {OpenShiftName}

IMPORTANT: To successfully deploy the Cluster Operator on {OpenShiftName} a user with `cluster-admin` role needs to be used (that is, like `system:admin`).

To deploy the Cluster Operator on {OpenShiftName}, you will first need to modify some of of the installation files according to the namespace the Cluster Operator is going to be installed in. 
To do this, execute the following command, replacing `<my-namespace>` with the correct namespace:

[source,shell]
sed -i 's/namespace: .*/namespace: <my-namespace>/' examples/install/cluster-operator/*ClusterRoleBinding*.yaml

Next, the following commands should be executed:

[source,shell]
oc create -f examples/install/cluster-operator
oc create -f examples/templates/cluster-operator

To verify whether the Cluster Operator has been deployed successfully, the {OpenShiftName} console or the following command
can be used:

[source,shell]
oc describe all

=== Kafka broker

{ProductName} uses the StatefulSets feature of {ProductPlatformName} to deploy Kafka brokers.
With StatefulSets, the pods receive a unique name and network identity and that makes it easier to identify the
individual Kafka broker pods and set their identity (broker ID). The deployment uses **regular** and **headless**
services:

- regular services can be used as bootstrap servers for Kafka clients
- headless services are needed to have DNS resolve the pods IP addresses directly

As well as Kafka, {ProductName} also installs a Zookeeper cluster and configures the Kafka brokers to connect to it. The
Zookeeper cluster also uses StatefulSets.

{ProductName} provides two flavors of Kafka broker deployment: **ephemeral** and **persistent**.

The **ephemeral** flavour is suitable only for development and testing purposes and not for production. The
ephemeral flavour uses `emptyDir` volumes for storing broker information (Zookeeper) and topics/partitions
(Kafka). Using `emptyDir` volume means that its content is strictly related to the pod life cycle (it is
deleted when the pod goes down). This makes the in-memory deployment well-suited to development and testing because
you do not have to provide persistent volumes.

The **persistent** flavour uses PersistentVolumes to store Zookeeper and Kafka data. The PersistentVolume is
acquired using a PersistentVolumeClaim â€“ that makes it independent of the actual type of the PersistentVolume. For
example, it can use
ifdef::Kubernetes[HostPath volumes on Minikube or]
Amazon EBS volumes in Amazon AWS deployments without any changes in the YAML files. The PersistentVolumeClaim can use
a StorageClass to trigger automatic volume provisioning.

To deploy a Kafka cluster, a `Kafka` resource with the cluster configuration has to be created.
Example resources and the details about the `Kafka` format are in <<kafka_assembly_details>>.

ifdef::Kubernetes[]
==== Deploying to {KubernetesName}

To deploy a Kafka broker on {KubernetesName}, the corresponding `Kafka` has to be created.
To create an ephemeral cluster using the provided example `Kafka`, the following command should be executed:

[source,shell]
kubectl apply -f examples/kafka/kafka-ephemeral.yaml

Another example `Kafka` is provided for a persistent Kafka cluster.
To deploy it, the following command should be run:

[source,shell]
kubectl apply -f examples/kafka/kafka-persistent.yaml
endif::Kubernetes[]

==== Deploying to {OpenShiftName}

For {OpenShiftName}, the Kafka broker is provided in the form of a template.
The cluster can be deployed from the template either using the command line or using the {OpenShiftName} console.
To create the ephemeral cluster, the following command should be executed:

[source,shell]
oc new-app strimzi-ephemeral

Similarly, to deploy a persistent Kafka cluster the following command should be run:

[source,shell]
oc new-app strimzi-persistent

=== Kafka Connect

The Cluster Operator can also deploy a https://kafka.apache.org/documentation/#connect[Kafka Connect] cluster which can be used with either of the Kafka broker deployments described above.
It is implemented as a Deployment with a configurable number of workers.
The default image currently contains only the Connectors distributed with Apache Kafka Connect: `FileStreamSinkConnector` and `FileStreamSourceConnector`.
The REST interface for managing the Kafka Connect cluster is exposed internally within the {ProductPlatformName} cluster as `kafka-connect` service on port `8083`.

Example `KafkaConnect` resources and the details about the `KafkaConnect` format for deploying Kafka Connect can be found in
<<kafka_connect_config_map_details>>.

ifdef::Kubernetes[]
==== Deploying to {KubernetesName}

To deploy Kafka Connect on {KubernetesName}, the corresponding `KafkaConnect` resource has to be created.
An example resource can be created using the following command:

[source,shell]
kubectl apply -f examples/kafka-connect/kafka-connect.yaml
endif::Kubernetes[]

==== Deploying to {OpenShiftName}

On {OpenShiftName}, Kafka Connect is provided in the form of a template. It can be deployed from the template either using the command line or using the {OpenShiftName} console.
To create a Kafka Connect cluster from the command line, the following command should be run:

[source,shell]
oc new-app strimzi-connect

==== Using Kafka Connect with additional plugins

{ProductName} Docker images for Kafka Connect contain, by default, only the `FileStreamSinkConnector` and `FileStreamSourceConnector` connectors which are part of Apache Kafka.

To facilitate deployment with 3rd party connectors, Kafka Connect is configured to automatically load all plugins/connectors which are present in the `/opt/kafka/plugins` directory during startup.
There are two ways of adding custom plugins into this directory:

- Using a custom Docker image
- Using the {OpenShiftName} build system with the {ProductName} S2I image

===== Create a new image based on our base image

{ProductName} provides its own Docker image for running Kafka Connect which can be found on {DockerRepository} as
`{DockerKafkaConnect}`.
This image could be used as a base image for building a new custom image with additional plugins.
The following steps describe the process for creating such a custom image:

1. Create a new `Dockerfile` which uses `{DockerKafkaConnect}` as the base image
+
[source,Dockerfile,subs="attributes"]
----
FROM {DockerKafkaConnect}
USER root:root
COPY ./my-plugin/ /opt/kafka/plugins/
USER {DockerImageUser}
----
2. Build the Docker image and upload it to the appropriate Docker repository
3. Use the new Docker image in the Kafka Connect deployment:
  - On {OpenShiftName}, the template parameters `IMAGE_REPO_NAME`, `IMAGE_NAME` and `IMAGE_TAG` can be changed to point to the new image when the Kafka Connect cluster is being deployed
ifdef::Kubernetes[  - On {KubernetesName}, the KafkaConnect resource has to be modified to use the new image]

===== Using {OpenShiftName} Build and S2I image

{OpenShiftName} supports https://docs.openshift.org/3.9/dev_guide/builds/index.html[Builds] which can be used together with the https://docs.openshift.org/3.9/creating_images/s2i.html#creating-images-s2i[Source-to-Image (S2I)] framework to create new Docker images. 
{OpenShiftName} Build takes a builder image with S2I support together with source code and binaries provided by the user and uses them to build a new Docker image.
The newly created Docker Image will be stored in {OpenShiftName}'s local Docker repository and can then be used in deployments. 
{ProductName} provides a Kafka Connect builder image which can be found on {DockerRepository} as `{DockerKafkaConnectS2I}` with such S2I support.
It takes user-provided binaries (with plugins and connectors) and creates a new Kafka Connect image. 
This enhanced Kafka Connect image can be used with our Kafka Connect deployment.

The S2I deployment is again provided as an {OpenShiftName} template. It can be deployed from the template either using the command
line or using the {OpenShiftName} console. To create Kafka Connect S2I cluster from the command line, the following command should
be run:

[source,shell]
oc new-app strimzi-connect-s2i

Once the cluster is deployed, a new Build can be triggered from the command line:

1. A directory with Kafka Connect plugins has to be prepared first. For example:
+
[source,shell]
----
$ tree ./my-plugins/
./my-plugins/
â”œâ”€â”€ debezium-connector-mongodb
â”‚Â Â  â”œâ”€â”€ bson-3.4.2.jar
â”‚Â Â  â”œâ”€â”€ CHANGELOG.md
â”‚Â Â  â”œâ”€â”€ CONTRIBUTE.md
â”‚Â Â  â”œâ”€â”€ COPYRIGHT.txt
â”‚Â Â  â”œâ”€â”€ debezium-connector-mongodb-0.7.1.jar
â”‚Â Â  â”œâ”€â”€ debezium-core-0.7.1.jar
â”‚Â Â  â”œâ”€â”€ LICENSE.txt
â”‚Â Â  â”œâ”€â”€ mongodb-driver-3.4.2.jar
â”‚Â Â  â”œâ”€â”€ mongodb-driver-core-3.4.2.jar
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ debezium-connector-mysql
â”‚Â Â  â”œâ”€â”€ CHANGELOG.md
â”‚Â Â  â”œâ”€â”€ CONTRIBUTE.md
â”‚Â Â  â”œâ”€â”€ COPYRIGHT.txt
â”‚Â Â  â”œâ”€â”€ debezium-connector-mysql-0.7.1.jar
â”‚Â Â  â”œâ”€â”€ debezium-core-0.7.1.jar
â”‚Â Â  â”œâ”€â”€ LICENSE.txt
â”‚Â Â  â”œâ”€â”€ mysql-binlog-connector-java-0.13.0.jar
â”‚Â Â  â”œâ”€â”€ mysql-connector-java-5.1.40.jar
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â””â”€â”€ wkb-1.0.2.jar
â””â”€â”€ debezium-connector-postgres
    â”œâ”€â”€ CHANGELOG.md
    â”œâ”€â”€ CONTRIBUTE.md
    â”œâ”€â”€ COPYRIGHT.txt
    â”œâ”€â”€ debezium-connector-postgres-0.7.1.jar
    â”œâ”€â”€ debezium-core-0.7.1.jar
    â”œâ”€â”€ LICENSE.txt
    â”œâ”€â”€ postgresql-42.0.0.jar
    â”œâ”€â”€ protobuf-java-2.6.1.jar
    â””â”€â”€ README.md
----

2. To start a new image build using the prepared directory, the following command has to be run:
+
[source,shell]
oc start-build my-connect-cluster-connect --from-dir ./my-plugins/
+
_The name of the build should be changed according to the cluster name of the deployed Kafka Connect cluster._

3. Once the build is finished, the new image will be used automatically by the Kafka Connect deployment.

=== Topic Operator

{ProductName} uses a component called the Topic Operator to manage topics in the Kafka cluster. The Topic Operator
is deployed as a process running inside a {ProductPlatformName} cluster. To create a new Kafka topic, a ConfigMap
with the related configuration (name, partitions, replication factor, ...) has to be created. Based on the information
in that ConfigMap, the Topic Operator will create a corresponding Kafka topic in the cluster.

Deleting a topic ConfigMap causes the deletion of the corresponding Kafka topic as well.

The Cluster Operator is able to deploy a Topic Operator, which can be configured in the `Kafka` resource.
Alternatively, it is possible to deploy a Topic Operator manually, rather than having it deployed
by the Cluster Operator.

==== Deploying through the Cluster Operator

To deploy the Topic Operator through the Cluster Operator, its configuration needs to be provided in the
`Kafka` resource in the `topicOperator` field as a JSON string.

For more information on the JSON configuration format see <<topic_operator_json_config>>.

==== Deploying standalone Topic Operator

If you are not going to deploy the Kafka cluster using the Cluster Operator but you already have a Kafka cluster deployed
on {ProductPlatformName}, it could be useful to deploy the Topic Operator using the provided YAML files.
In that case you can still leverage on the Topic Operator features of managing Kafka topics through related ConfigMaps.

ifdef::Kubernetes[]
===== Deploying to {KubernetesName}

To deploy the Topic Operator on {KubernetesName} (not through the Cluster Operator), the following command should be executed:

[source,shell]
kubectl create -f examples/install/topic-operator.yaml

To verify whether the Topic Operator has been deployed successfully, the {KubernetesName} Dashboard or the following
command can be used:

[source,shell]
kubectl describe all
endif::Kubernetes[]

===== Deploying to {OpenShiftName}

To deploy the Topic Operator on {OpenShiftName} (not through the Cluster Operator), the following command should be executed:

[source,shell]
oc create -f examples/install/topic-operator

To verify whether the Topic Operator has been deployed successfully, the {OpenShiftName} console or the following command
can be used:

[source,shell]
oc describe all

==== Topic ConfigMap

When the Topic Operator is deployed by the Cluster Operator it will be configured to watch
for "topic ConfigMaps" which are those with the following labels:

[source,yaml]
strimzi.io/cluster: <cluster-name>
strimzi.io/kind: topic

NOTE: When the Topic Operator is deployed manually the `strimzi.io/cluster` label is not necessary.

The topic ConfigMap contains the topic configuration in a specific format. The ConfigMap format is described in <<topic_config_map_details>>.

==== Logging
The `logging` field allows the configuration of loggers. These loggers are listed below.
[source]
rootLogger.level

For information on the logging options and examples of how to set logging, see <<logging_examples, logging examples>> for Kafka.

When using external ConfigMap remember to place your custom ConfigMap under `log4j2.properties` key.
