== Cluster Operator

The Cluster Operator is in charge of deploying a Kafka cluster alongside a Zookeeper ensemble.
As part of the Kafka cluster, it can also deploy the topic operator which provides operator-style topic management via ConfigMaps.
The Cluster Operator is also able to deploy a Kafka Connect cluster which connects to an existing Kafka cluster.
On {OpenShiftName} such a cluster can be deployed using the Source2Image feature, providing an easy way of including more connectors.

.Example Architecture diagram of the Cluster Operator.
image::cluster_operator.png[Cluster Operator]

When the Cluster Operator is up, it starts to "watch" for certain {ProductPlatformName} resources containing the desired Kafka or Kafka Connect cluster configuration.
A `Kafka` resource is used for Kafka cluster configuration, and a `KafkaConnect` resource is used for Kafka Connect cluster configuration.

When a new desired resource (that is, a `Kafka` or `KafkaConnect` resource) is created in the {ProductPlatformName} cluster, the operator gets the cluster configuration from the desired resource and starts creating a new Kafka or Kafka Connect cluster by creating the necessary other {ProductPlatformName} resources, such as StatefulSets, Services, ConfigMaps, and so on.

Every time the desired resource is updated by the user, the operator performs corresponding updates on the {ProductPlatformName} resources which make up the Kafka or Kafka Connect cluster.
Resources are either patched or deleted and then re-created in order to make the Kafka or Kafka Connect cluster reflect the state of the desired cluster resource.
This might cause a rolling update which might lead to service disruption.

Finally, when the desired resource is deleted, the operator starts to un-deploy the cluster deleting all the related {ProductPlatformName} resources.

=== Reconciliation

Although the operator reacts to all notifications about the desired cluster resources received from the {ProductPlatformName} cluster,
if the operator is not running, or if a notification is not received for any reason, the desired resources will get out of sync with the state of the running {ProductPlatformName} cluster.

In order to handle failovers properly, a periodic reconciliation process is executed by the Cluster Operator so that it can compare the state of the desired resources with the current cluster deployments in order to have a consistent state across all of them.

[[kafka_assembly_details]]
=== Format of the `Kafka` resource

The full API is described in <<kafka_resource_reference>>.

Whatever other labels are applied to the desired Kafka resource will also be applied to the {ProductPlatformName} resources making up the Kafka cluster.
This provides a convenient mechanism for those resources to be labelled in whatever way the user requires.

[[kafka_config_map_details]]
==== Kafka

In order to configure a Kafka cluster deployment, it is possible to specify the following fields in Kafka resource (a dot is used to denote a nested YAML object):

`spec.kafka.replicas`::
The number of Kafka broker nodes.
Default is 3.
`spec.kafka.image`::
The Docker image to be used by the Kafka brokers.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_KAFKA_IMAGE,STRIMZI_DEFAULT_KAFKA_IMAGE>>` environment variable of the Cluster Operator.
`spec.kafka.brokerRackInitImage`::
The Docker image to be used by the init container which does some initial configuration work (that is, rack support).
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_KAFKA_INIT_IMAGE,STRIMZI_DEFAULT_KAFKA_INIT_IMAGE>>` environment variable of the Cluster Operator.
`spec.kafka.livenessProbe.initialDelaySeconds`::
The initial delay for the liveness probe for each Kafka broker node. 
Default is 15.
`spec.kafka.livenessProbe.timeoutSeconds`::
The timeout on the liveness probe for each Kafka broker node. 
Default is 5.
`spec.kafka.readinessProbe.initialDelaySeconds`::
The initial delay for the readiness probe for each Kafka broker node. 
Default is 15.
`spec.kafka.readinessProbe.timeoutSeconds`::
The timeout on the readiness probe for each Kafka broker node. 
Default is 5.
`spec.kafka.config`::
The Kafka broker configuration. 
See section <<kafka_configuration_json_config>> for more details.
`spec.kafka.storage`::
The storage configuration for the Kafka broker nodes. 
See section <<storage_configuration_json_config>> for more details.
`spec.kafka.metrics`::
The JMX exporter configuration for exposing metrics from Kafka broker nodes.
When this field is absent no metrics will be exposed.
[[spec.kafka.logging]]`spec.kafka.logging`::
An object that specifies inline logging levels or the name of external config map that specifies the logging levels.
When this field is absent default values are used.
List of loggers which can be set:
[source]
kafka.root.logger.level
log4j.logger.org.I0Itec.zkclient.ZkClient
log4j.logger.org.apache.zookeeper
log4j.logger.kafka
log4j.logger.org.apache.kafka
log4j.logger.kafka.request.logger
log4j.logger.kafka.network.Processor
log4j.logger.kafka.server.KafkaApis
log4j.logger.kafka.network.RequestChannel$
log4j.logger.kafka.controller
log4j.logger.kafka.log.LogCleaner
log4j.logger.state.change.logger
log4j.logger.kafka.authorizer.logger

`spec.kafka.resources`::
The resource limits and requests for Kafka broker containers. 
The accepted format is described in the <<resources_json_config>> section.
`spec.kafka.jvmOptions`::
An object allowing the JVM running Kafka to be configured.
The accepted format is described in the <<jvm_json_config>> section.
`spec.kafka.rack`::
An object allowing the Kafka rack feature to be configured and used in rack-aware partition assignment for fault tolerance.
For information about the accepted JSON format, see <<kafka_rack>> section.
`spec.kafka.affinity`::
An object allowing control over how the Kafka pods are scheduled to nodes.
The format of the corresponding key is the same as the content supported in the Pod `affinity` in {ProductPlatformName}.
See section <<affinity>> for more details.
`spec.kafka.tlsSidecar.image`::
The Docker image to be used by the sidecar container which provides TLS support for Kafka brokers.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE,STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE>>` environment variable of the Cluster Operator.
`spec.kafka.tlsSidecar.resources`::
An object configuring the resource limits and requests for the sidecar container which provides TLS support for Kafka brokers.
For information about the accepted JSON format, see <<resources_json_config>>.
`spec.zookeeper.replicas`::
The number of Zookeeper nodes.
`spec.zookeeper.image`::
The Docker image to be used by the Zookeeper nodes.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_ZOOKEEPER_IMAGE,STRIMZI_DEFAULT_ZOOKEEPER_IMAGE>>` environment variable of the Cluster Operator.
`spec.zookeeper.livenessProbe.initialDelaySeconds`::
The initial delay for the liveness probe for each Zookeeper node. 
Default is 15.
`spec.zookeeper.livenessProbe.initialDelaySeconds`::
The timeout on the liveness probe for each Zookeeper node. 
Default is 5.
`spec.zookeeper.readinessProbe.initialDelaySeconds`::
The initial delay for the readiness probe for each Zookeeper node. 
Default is 15.
`spec.zookeeper.readinessProbe.initialDelaySeconds`::
The timeout on the readiness probe for each Zookeeper node. 
Default is 5.
`spec.zookeeper.config`::
The Zookeeper configuration. See section <<zookeeper_configuration_json_config>> for more details.
`spec.zookeeper.storage`::
The storage configuration for the Zookeeper nodes. See section <<storage_configuration_json_config>> for more details.
`spec.zookeeper.metrics`::
The JMX exporter configuration for exposing metrics from Zookeeper nodes.
When this field is absent no metrics will be exposed.
[[spec.zookeeper.logging]]`spec.zookeeper.logging`::
An object that specifies inline logging levels or the name of external config map that specifies the logging levels.
When this field is absent default values are used.
List of loggers which can be set:
[source]
zookeeper.root.logger

`spec.zookeeper.resources`::
An object configuring the resource limits and requests for Zookeeper broker containers.
For information about the accepted JSON format, see <<resources_json_config>> section.
`spec.zookeeper.jvmOptions`::
An object allowing the JVM running Zookeeper to be configured. 
For information about the accepted JSON format, see <<jvm_json_config>> section.
`spec.zookeeper.affinity`::
An object allowing control over how the Zookeeper pods are scheduled to nodes.
The format of the corresponding key is the same as the content supported in the Pod `affinity` in {ProductPlatformName}.
See section <<affinity>> for more details.
`spec.zookeeper.tlsSidecar.image`::
The Docker image to be used by the sidecar container which provides TLS support for Zookeeper nodes.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_TLS_SIDECAR_ZOOKEEPER_IMAGE,STRIMZI_DEFAULT_TLS_SIDECAR_ZOOKEEPER_IMAGE>>` environment variable of the Cluster Operator.
`spec.zookeeper.tlsSidecar.resources`::
An object configuring the resource limits and requests for the sidecar container which provides TLS support for Zookeeper nodes.
For information about the accepted JSON format, see <<resources_json_config>>.
`spec.topicOperator`::
An object representing the topic operator configuration. 
See the <<topic_operator_json_config>> documentation for further details.
More info about the topic operator in the related <<Topic operator>> documentation page.
 
The following is an example of a Kafka resource.

.Example `Kafka` resource
[source,yaml,options="nowrap",subs="attributes"]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    replicas: 3
    image: "{DockerKafka}"
    kafka-healthcheck-delay: "15"
    kafka-healthcheck-timeout: "5"
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: ephemeral
    metrics:
      {
        "lowercaseOutputName": true,
        "rules": [
            {
              "pattern": "kafka.server&lt;type=(.+), name=(.+)PerSec\\w*&gt;&lt;&gt;Count",
              "name": "kafka_server_$1_$2_total"
            },
            {
              "pattern": "kafka.server&lt;type=(.+), name=(.+)PerSec\\w*, topic=(.+)&gt;&lt;&gt;Count",
              "name": "kafka_server_$1_$2_total",
              "labels":
              {
                "topic": "$3"
              }
            }
        ]
      }
    logging: 
      type: external
      name: customConfigMap
  zookeeper:
    replicas: 1
    image: {DockerZookeeper}
    healthcheck-delay: "15"
    healthcheck-timeout: "5"
    config:
      timeTick: 2000,
      initLimit: 5,
      syncLimit: 2,
      autopurge.purgeInterval: 1
    storage:
      type: ephemeral
    metrics:
      {
        "lowercaseOutputName": true
      }
    logging: 
      type : inline
      loggers : 
        zookeeper.root.logger: INFO
----

The resources created by the Cluster Operator in the {ProductPlatformName} cluster will be the following :

`[cluster-name]-zookeeper`:: StatefulSet which is in charge of managing the Zookeeper node pods
`[cluster-name]-kafka`:: StatefulSet which is in charge of managing the Kafka broker pods
`[cluster-name]-zookeeper-nodes`:: Service needed to have DNS resolve the Zookeeper pods IP addresses directly
`[cluster-name]-kafka-brokers`:: Service needed to have DNS resolve the Kafka broker pods IP addresses directly
`[cluster-name]-zookeeper-client`:: Service used by Kafka brokers to connect to Zookeeper nodes as clients
`[cluster-name]-kafka-bootstrap`:: Service can be used as bootstrap servers for Kafka clients
`[cluster-name]-zookeeper-metrics-config`:: ConfigMap which contains the Zookeeper metrics configuration and mounted as a volume by the Zookeeper node pods
`[cluster-name]-kafka-metrics-config`:: ConfigMap which contains the Kafka metrics configuration and mounted as a volume by the Kafka broker pods
`[cluster-name]-zookeeper-config`:: 
ConfigMap which contains the Zookeeper ancillary configuration and is mounted as a volume by the Zookeeper node pods
`[cluster-name]-kafka-config`:: 
ConfigMap which contains the Kafka ancillary configuration and is mounted as a volume by the Kafka broker pods

[[kafka_configuration_json_config]]
===== Kafka Configuration

The `spec.kafka.config` object allows detailed configuration of Apache Kafka. This field should contain a JSON object with Kafka
configuration options as keys. The values could be in one of the following JSON types:

* String
* Number
* Boolean

The `spec.kafka.config` object supports all Kafka configuration options with the exception of options related to:

* Security (Encryption, Authentication and Authorization)
* Listener configuration
* Broker ID configuration
* Configuration of log data directories
* Inter-broker communication
* Zookeeper connectivity

Specifically, all configuration options with keys starting with one of the following strings will be ignored:

* `listeners`
* `advertised.`
* `broker.`
* `listener.`
* `host.name`
* `port`
* `inter.broker.listener.name`
* `sasl.`
* `ssl.`
* `security.`
* `password.`
* `principal.builder.class`
* `log.dir`
* `zookeeper.connect`
* `zookeeper.set.acl`
* `authorizer.`
* `super.user`

All other options will be passed to Kafka.
A list of all the available options can be found on the http://kafka.apache.org/11/documentation.html#brokerconfigs[Kafka website].
An example `spec.kafka.config` field is provided below.

.Example fragment of a `Kafka` resource specifying Kafka configuration
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    config:
      num.partitions: 1,
      num.recovery.threads.per.data.dir: 1,
      default.replication.factor: 3,
      offsets.topic.replication.factor: 3,
      transaction.state.log.replication.factor: 3,
      transaction.state.log.min.isr: 1,
      log.retention.hours: 168,
      log.segment.bytes: 1073741824,
      log.retention.check.interval.ms: 300000,
      num.network.threads: 3,
      num.io.threads: 8,
      socket.send.buffer.bytes: 102400,
      socket.receive.buffer.bytes: 102400,
      socket.request.max.bytes: 104857600,
      group.initial.rebalance.delay.ms: 0
    # ...
----

NOTE:: The Cluster Operator does not validate keys or values in the provided `config` object.
When invalid configuration is provided, the Kafka cluster might not start or might become unstable.
In such cases, the configuration in the `spec.kafka.config` object should be fixed and the cluster operator will roll out the new configuration to all Kafka brokers.

[[zookeeper_configuration_json_config]]
===== Zookeeper Configuration

The `spec.zookeeper.config` object allows detailed configuration of Apache Zookeeper. This field should contain a JSON object
with Zookeeper configuration options as keys. The values could be in one of the following JSON types:

* String
* Number
* Boolean

The `spec.zookeeper.config` object supports all Zookeeper configuration options with the exception of options related to:

* Security (Encryption, Authentication and Authorization)
* Listener configuration
* Configuration of data directories
* Zookeeper cluster composition

Specifically, all configuration options with keys starting with one of the following strings will be ignored:

* `server.`
* `dataDir`
* `dataLogDir`
* `clientPort`
* `authProvider`
* `quorum.auth`
* `requireClientAuthScheme`

All other options will be passed to Zookeeper.
A list of all the available options can be found on the http://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html[Zookeeper website].
An example `spec.zookeeper.config` object is provided below.

.Example fragment of a `Kafka` resource specifying Zookeeper configuration
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  zookeeper:
    # ...
    config:
      timeTick: 2000,
      initLimit: 5,
      syncLimit: 2,
      quorumListenOnAllIPs: true,
      maxClientCnxns: 0,
      autopurge.snapRetainCount: 3,
      autopurge.purgeInterval: 1
    # ...
----

Selected options have default values:

* `timeTick` with default value `2000`
* `initLimit` with default value `5`
* `syncLimit` with default value `2`
* `autopurge.purgeInterval` with default value `1`

These options will be automatically configured in case they are not present in the `spec.zookeeper.config` object.

NOTE:: The Cluster Operator does not validate keys or values in the provided `config` object.
When invalid configuration is provided, the Zookeeper cluster might not start or might become unstable.
In such cases, the configuration in the `spec.zookeeper.config` object should be fixed and the cluster operator will roll out the new configuration to all Zookeeper nodes.

[[storage_configuration_json_config]]
===== Storage

Both Kafka and Zookeeper save data to files.

{ProductName} allows to save such data in an "ephemeral" way (using `emptyDir`) or in a "persistent-claim" way using persistent volumes.
It is possible to provide the storage configuration in the `spec.kafka.storage` and `spec.zookeeper.storage` objects.

IMPORTANT: The `spec.kafka.storage` and `spec.zookeeper.storage` objects cannot be changed when the cluster is up.

The storage object has a mandatory `type` field for specifying the type of storage to use which must be either "ephemeral" or "persistent-claim".

The "ephemeral" storage is really simple to configure.

.Example fragment of a `Kafka` resource using `ephemeral` storage for Kafka pods
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: ephemeral
    # ...
----

WARNING: If the Zookeeper cluster is deployed using "ephemeral" storage, the Kafka brokers can have problems dealing with Zookeeper node restarts which could happen via updates in the Kafka resource.

In case of "persistent-claim" type the following fields can be provided as well:

`size` (required):: 
defines the size of the persistent volume claim, for example, "1Gi".

`class` (optional):: 
the {ProductPlatformName} https://kubernetes.io/docs/concepts/storage/storage-classes/[storage class] to use for dynamic volume allocation.

`selector` (optional)::
allows to select a specific persistent volume to use. 
It contains a `matchLabels` field which contains key:value pairs representing labels for selecting such a volume.

`delete-claim` (optional)::
boolean value which specifies if the persistent volume claim has to be deleted when the cluster is undeployed.
Default is `false`.

.Example fragment of a `Kafka` resource configuring Kafka with `persistent-storage` and 1Gi `size`
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: persistent-claim
      size: 1Gi
    # ...
----

The following example demonstrates use of a storage class.

.Example fragment of a `Kafka` resource configuring Kafka with `persistent-storage` using a storage class
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: persistent-claim
      size: 1Gi
      class: my-storage-class
    # ...
----

Finally, a `selector` can be used in order to select a specific labelled persistent volume which provides some needed features (such as an SSD)

.Example fragment of a `Kafka` resource configuring Kafka with "match labels" selector
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: persistent-claim
      size: 1Gi
      selector:
        matchLabels:
          "hdd-type": "ssd"
      deleteClaim: true
    # ...
----

When the "persistent-claim" is used, other than the resources already described in the <<Kafka>> section, the following resources are generated :

`data-[cluster-name]-kafka-[idx]`:: 
Persistent Volume Claim for the volume used for storing data for the Kafka broker pod `[idx]`.

`data-[cluster-name]-zookeeper-[idx]`:: 
Persistent Volume Claim for the volume used for storing data for the Zookeeper node pod `[idx]`.

See <<kafka.strimzi.io-v1alpha1-type-EphemeralStorage>> and <<kafka.strimzi.io-v1alpha1-type-PersistentClaimStorage>> for further details.

===== Metrics

{ProductName} uses the [Prometheus JMX exporter](https://github.com/prometheus/jmx_exporter) in order to expose metrics on each node. 
It is possible to configure a `metrics` object in the `kafka` and `zookeeper` objects in `Kafka` resources, and likewise a `metrics` object in the `spec` of `KafkaConnect` resources. 
In all cases the `metrics` object should be the configuration for the JMX exporter. 
You can find more information on how to use it in the corresponding GitHub repo.

For more information about using the metrics with Prometheus and Grafana, see <<_metrics>>

===== [[logging_examples]]Logging
The `logging` field allows the configuration of loggers. These loggers for Zookeeper and Kafka are available in the <<spec.zookeeper.logging,`spec.zookeeper.logging`>> and <<spec.kafka.logging,`spec.kafka.logging`>> sections respectively.

The setting can be done in one of two ways. Either by specifying the loggers and their levels directly or by using a custom config map.
An example would look like this:

[source,yaml]
----
  logging:
    type: inline
    loggers:
      logger.name: "INFO"
----
The `INFO` can be replaced with any log4j logger level. The available logger levels are `INFO`, `ERROR`, `WARN`, `TRACE`, `DEBUG`, `FATAL` or `OFF`.
The informations about log levels can be found in the https://logging.apache.org/log4j/2.x/manual/customloglevels.html[log4j manual].

[source,yaml]
----
  logging:
    type: external
    name: customConfigMap
----

When using external ConfigMap remember to place your custom ConfigMap under `log4j.properties` key.

The difference between these two options is that the latter is not validated and does not support default values.
That means the user can supply any logging configuration, even if it is incorrect.
The first option supports default values.


[[resources_json_config]]
===== Resource limits and requests

It is possible to configure {ProductPlatformName} resource limits and requests on for the `kafka`, `zookeeper` and `topicOperator` objects in the `Kafka` resource and for for the `spec` object of the `KafkaConnect resource.
The object may have a `requests` and a `limits` property, each having the same schema, consisting of `cpu` and `memory` properties.
The {ProductPlatformName} syntax is used for the values of `cpu` and `memory`.

.Example fragment of a `Kafka` resource configuring resource limits and requests for the Kafka pods
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    resources:
      requests: 
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "1"
        memory: "2Gi"
    # ...
----

:k8s-docs-version: v1-7
:k8s-resource-request-limit-docs-link: https://{k8s-docs-version}.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/

`requests.memory`::
the memory request for the container, corresponding directly to {k8s-resource-request-limit-docs-link}[`spec.containers[\].resources.requests.memory`] setting.
{ProductPlatformName} will ensure the containers have at least this much memory by running the pod on a node with at
least as much free memory as all the containers require. Optional with no default.
`requests.cpu`::
the cpu request for the container, corresponding directly to {k8s-resource-request-limit-docs-link}[`spec.containers[\].resources.requests.cpu`] setting.
{ProductPlatformName} will ensure the containers have at least this much CPU by running the pod on a node with at least
as much uncommitted CPU as all the containers require. Optional with no default.
`limits.memory`::
the memory limit for the container, corresponding directly to {k8s-resource-request-limit-docs-link}[`spec.containers[\].resources.limits.memory`] setting.
{ProductPlatformName} will limit the containers to this much memory, potentially terminating their pod if they use more.
Optional with no default.
`limits.cpu`::
the cpu limit for the container, corresponding directly to {k8s-resource-request-limit-docs-link}[`spec.containers[\].resources.limits.cpu`] setting.
{ProductPlatformName} will cap the containers CPU usage to this limit. Optional with no default.

More details about resource limits and requests can be found on {k8s-resource-request-limit-docs-link}[{KubernetesName} website].

====== Minimum Resource Requirements

Testing has shown that the Cluster Operator functions adequately with 256Mi of memory and 200m CPU when watching two clusters.
It is therefore recommended to use these as a minimum when configuring resource requests and not to run it with lower limits than these.
Configuring more generous limits is recommended, especially when it is controlling multiple clusters.


[[jvm_json_config]]
===== JVM Options

It is possible to configure a subset of available JVM options on Kafka, Zookeeper and Kafka Connect containers.
The object has a property for each JVM (`java`) option which can be configured:

`-Xmx`::
The maximum heap size. See the <<setting_xmx>> section for further details.

`-Xms`::
The initial heap size.
Setting the same value for initial and maximum (`-Xmx`) heap sizes avoids the JVM having to allocate memory after startup,
at the cost of possibly allocating more heap than is really needed. For Kafka and Zookeeper pods such allocation could
cause unwanted latency. For Kafka Connect avoiding over allocation may be the more important concern, especially in
distributed mode where the effects of over-allocation will be multiplied by the number of consumers.

NOTE: The units accepted by JVM settings such as `-Xmx` and `-Xms` are those accepted by the JDK `java`
binary in the corresponding image. Accordingly, `1g` or `1G` means 1,073,741,824 bytes, and `Gi` is not a valid unit
suffix. This is in contrast to the units used for <<resources_json_config,memory limits and requests>>, which follow the
{ProductPlatformName} convention where `1G` means 1,000,000,000 bytes, and `1Gi` means 1,073,741,824 bytes

.Example fragment of a `Kafka` resource configuring `jvmOptions`
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    jvmOptions:
      "-Xmx": "2g"
      "-Xms": "2g"
    # ...
----

In the above example, the JVM will use 2 GiB (=2,147,483,648 bytes) for its heap.
Its total memory usage will be approximately 8GiB.

`-server`::
Selects the server JVM. This option can be set to true or false. Optional.

`-XX`::
A JSON Object for configuring advanced runtime options of a JVM. Optional

The `-server` and `-XX` options are used to configure the `KAFKA_JVM_PERFORMANCE_OPTS` option of Apache Kafka.

.More sophisticated example fragment of a `Kafka` resource configuring `jvmOptions`
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    "-server": true,
    "-XX": 
      "UseG1GC": true,
      "MaxGCPauseMillis": 20,
      "InitiatingHeapOccupancyPercent": 35,
      "ExplicitGCInvokesConcurrent": true,
      "UseParNewGC": false
----

The example configuration above will result in the following JVM options:

[source]
----
-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:-UseParNewGC
----

When neither of the two options (`-server` and `-XX`) is specified, the default Apache Kafka configuration of `KAFKA_JVM_PERFORMANCE_OPTS` will be used.

[[setting_xmx]]
====== Setting `-Xmx`

The default value used for `-Xmx` depends on whether there is a <<resources_json_config,memory limit>> for the container:

* If there is a memory limit, the JVM's maximum memory will be limited according to the kind of pod (Kafka, Zookeeper,
Topic Operator) to an appropriate value less than the limit.
* Otherwise, when there is no memory limit, the JVM's maximum memory will be set according to the kind of pod and the
RAM available to the container.

[IMPORTANT]
====
Setting `-Xmx` explicitly is requires some care:

* The JVM's overall memory usage will be approximately 4 × the maximum heap, as configured by `-Xmx`.

* If `-Xmx` is set without also setting an appropriate {ProductPlatformName}
memory limit, it is possible that the container will be killed should the {ProductPlatformName} node
experience memory pressure (from other Pods running on it).

* If `-Xmx` is set without also setting an appropriate {ProductPlatformName}
memory request, it is possible that the container will scheduled to a node with insufficient memory.
In this case the container will start but crash (immediately if `-Xms` is set to `-Xmx`, or some later time if not).

====

When setting `-Xmx` explicitly, it is recommended to:

* set the memory request and the memory limit to the same value,
* use a memory request that is at least 4.5 × the `-Xmx`,
* consider setting `-Xms` to the same value as `-Xms`.

Furthermore, containers doing lots of disk I/O (such as Kafka broker containers) will need to leave some memory available
for use as operating system page cache. On such containers, the request memory should be substantially more than the
memory used by the JVM.

[[kafka_rack]]
===== Kafka rack

It is possible to enable Kafka rack-awareness (more information can be found on the {KafkaRacks})
by specifying the `rack` object in the `spec.kafka` object of the `Kafka` resource.
The `rack` object has one mandatory field named `topologyKey`.
This key needs to match one of the labels assigned to the {ProductPlatformName} cluster nodes.
The label is used by {ProductPlatformName} when scheduling Kafka broker pods to nodes.
If the {ProductPlatformName} cluster is running on a cloud provider platform, that label should represent the availability zone where the node is running.
Usually, the nodes are labeled with `failure-domain.beta.kubernetes.io/zone` that can be easily used as `topologyKey` value.
This will have the effect of spreading the broker pods across zones, and also setting the brokers `broker.rack` configuration parameter.

.Example fragment of a `Kafka` resource configuring the `rack`
[source,json]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    rack:
      topologyKey: failure-domain.beta.kubernetes.io/zone
    # ...
----

In the above example, the `failure-domain.beta.kubernetes.io/zone` node label will be used for scheduling Kafka broker Pods.

[[affinity]]
===== Node and Pod Affinity

Node and Pod Affinity provide a flexible mechanism to guide the scheduling of pods to nodes by {ProductPlatformName}.
Node affinity can be used so that broker pods are preferentially scheduled to nodes with fast disks, for example.
Similarly, pod affinity could be used to try to schedule Kafka clients on the same nodes as Kafka brokers.
More information can be found on the {K8sAffinity}.

The format of the corresponding key is the same as the content supported in the Pod `affinity` in {ProductPlatformName}, that is: `nodeAffinity`, `podAffinity` and `podAntiAffinity`.

.Example fragment of a `Kafka` resource configured with `nodeAffinity`
[source,yaml]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    affinity: 
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/e2e-az-name
              operator: In
              values:
              - e2e-az1
              - e2e-az2
    # ...
----

NOTE: When using both `affinity` and <<kafka_rack,`rack`>> be aware that `rack` uses a pod anti-affinity.
This is necessary so that broker pods are scheduled in different failure domains, as specified via the `topologyKey`.
This anti-affinity will not be present in the `Kafka` resource's `affinity`, but is still present on the StatefulSet and thus will still be considered by the scheduler.

[[topic_operator_json_config]]
===== Topic Operator

Alongside the Kafka cluster and the Zookeeper ensemble, the Cluster Operator can also deploy the topic operator.
In order to do that, a `spec.topicOperator` object has to be included in the `Kafka` resource.
This object contains the topic operator configuration.
Without this object, the Cluster Operator does not deploy the topic operator. 
It is still possible to deploy the topic operator by creating appropriate {ProductPlatformName} resources.

The YAML representation of the 'topicOperator` has no mandatory fields and if the value is an empty object
(just "{ }"), the Cluster Operator will deploy the topic operator with a default configuration.

The configurable fields are the following :

`image`::
The Docker image to be used by the Topic Operator.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_TOPIC_operator_IMAGE,STRIMZI_DEFAULT_TOPIC_operator_IMAGE>>` environment variable of the Cluster Operator.
`watchedNamespace`::
The {ProductPlatformName} namespace in which the topic operator watches for topic ConfigMaps. Default is the namespace
where the topic operator is running.
`reconciliationIntervalMs`::
The interval between periodic reconciliations in milliseconds. Default is 900000 (15 minutes).
`zookeeperSessionTimeoutMs`::
The Zookeeper session timeout in milliseconds. Default is 20000 milliseconds (20 seconds).
`topicMetadataMaxAttempts`::
The number of attempts for getting topics metadata from Kafka. The time between each attempt is defined as an exponential
back-off. You might want to increase this value when topic creation could take more time due to its larger size (i.e.
many partitions / replicas). Default is `6`.
`resources`::
An object configuring the resource limits and requests for the topic operator container. The accepted JSON format is
described in the <<resources_json_config>> section.
`affinity`::
Node and Pod affinity for the Topic Operator, as described in the <<affinity>> section.
The format of the corresponding key is the same as the content supported in the Pod `affinity` in {ProductPlatformName}.
`tlsSidecar.image`::
The Docker image to be used by the sidecar container which provides TLS support for Topic Operator.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_TLS_SIDECAR_TOPIC_OPERATOR_IMAGE,STRIMZI_DEFAULT_TLS_SIDECAR_TOPIC_OPERATOR_IMAGE>>` environment variable of the Cluster Operator.
`tlsSidecar.resources`::
An object configuring the resource limits and requests for the sidecar container which provides TLS support for the Topic Operator.
For information about the accepted JSON format, see <<resources_json_config>>.

.Example Topic Operator JSON configuration
[source,json]
----
{ "reconciliationIntervalMs": "900000", "zookeeperSessionTimeoutMs": "20000" }
----

More information about these configuration parameters in the related <<Topic Operator>> documentation page.

[[kafka_connect_config_map_details]]
==== Kafka Connect

The operator watches for `KafkaConnect` resource in order to find and get configuration for a Kafka Connect cluster to deploy.

The `KafkaConnectS2I` resource provides configuration for a Kafka Connect cluster deployment using Build and Source2Image features (works only with {OpenShiftName})

Whatever other labels are applied to the `KafkaConnect` or `KafkaConnectS2I` resources will also be applied to the {ProductPlatformName} resources making up the Kafka Connect cluster.
This provides a convenient mechanism for those resource to be labelled
in whatever way the user requires.

The `KafkaConnect` resource supports the following within its `spec`:

`nodes`:: 
Number of Kafka Connect worker nodes. Default is 1.

`image`:: 
The Docker image to be used by the Kafka Connect workers.
The default value is determined by the value specified in the `<<STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE,STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE>>` environment variable of the Cluster Operator.
If S2I is used (only on {OpenShiftName}), then it should be the related S2I image.

`healthcheck-delay`::
The initial delay for the liveness and readiness probes for each Kafka Connect worker node. 
Default is 60.

`healthcheck-timeout`::
The timeout on the liveness and readiness probes for each Kafka Connect worker node. 
Default is 5.

`connect-config`::
A JSON string with Kafka Connect configuration. 
See section <<kafka_connect_configuration_json_config>> for more details.

`metrics-config`::
A JSON string representing the JMX exporter configuration for exposing metrics from Kafka Connect nodes.
When this field is absent no metrics will be exposed.

`resources`:: 
A JSON string configuring the resource limits and requests for Kafka Connect containers.
For information about the accepted JSON format, see <<resources_json_config>> section.

`jvmOptions`:: 
A JSON string allowing the JVM running Kafka Connect to be configured.
For information about the accepted JSON format, see <<jvm_json_config>> section.

`affinity`::
Node and Pod affinity for the Kafka Connect pods, as described in the <<affinity>> section.
The format of the corresponding key is the same as the content supported in the Pod `affinity` in {ProductPlatformName}.

The following is an example of a `KafkaConnect` resource.

.Example KafkaConnect resource
[source,yaml,options="nowrap",subs="attributes"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  nodes: 1
  image: {DockerKafkaConnect}
  readinessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  config: 
    bootstrap.servers: my-cluster-kafka-bootstrap:9092
----

The resources created by the Cluster Operator into the {ProductPlatformName} cluster will be the following :

[connect-cluster-name]-connect::
Deployment which is in charge to create the Kafka Connect worker node pods.
[connect-cluster-name]-connect-api::
Service which exposes the REST interface for managing the Kafka Connect cluster.
[connect-cluster-name]-metrics-config::
ConfigMap which contains the Kafka Connect metrics configuration and is mounted as a volume by the Kafka Connect pods.

[[kafka_connect_configuration_json_config]]
===== Kafka Connect configuration

The `spec.config` object of the `KafkaConnect` and `KafkaConnectS2I` resources allows detailed configuration of Apache Kafka Connect. 
This object should contain the Kafka Connect configuration options as keys. The values could be in one of the following JSON types:


* String
* Number
* Boolean

The `config` object supports all Kafka Connect configuration options with the exception of options related to:

* Security (Encryption, Authentication and Authorization)
* Listener / REST interface configuration
* Plugin path configuration

Specifically, all configuration options with keys starting with one of the following strings will be ignored:

* `ssl.`
* `sasl.`
* `security.`
* `listeners`
* `plugin.path`
* `rest.`

All other options will be passed to Kafka Connect. A list of all the available options can be found on the
http://kafka.apache.org/11/documentation.html#connectconfigs[Kafka website]. An example `config` field is provided
below.

.Example Kafka Connect configuration
[source,json]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  config:
    bootstrap.servers: my-cluster-kafka:9092
    group.id: my-connect-cluster
    offset.storage.topic: my-connect-cluster-offsets
    config.storage.topic: my-connect-cluster-configs
    status.storage.topic: my-connect-cluster-status
    key.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: true
    value.converter.schemas.enable: true
    internal.key.converter: org.apache.kafka.connect.json.JsonConverter
    internal.value.converter: org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable: false
    internal.value.converter.schemas.enable: false
    config.storage.replication.factor: 3
    offset.storage.replication.factor: 3
    status.storage.replication.factor: 3
}
----

Selected options have default values:

* `group.id` with default value `connect-cluster`
* `offset.storage.topic` with default value `connect-cluster-offsets`
* `config.storage.topic` with default value `connect-cluster-configs`
* `status.storage.topic` with default value `connect-cluster-status`
* `key.converter` with default value `org.apache.kafka.connect.json.JsonConverter`
* `value.converter` with default value `org.apache.kafka.connect.json.JsonConverter`
* `internal.key.converter` with default value `org.apache.kafka.connect.json.JsonConverter`
* `internal.value.converter` with default value `org.apache.kafka.connect.json.JsonConverter`
* `internal.key.converter.schemas.enable` with default value `false`
* `internal.value.converter.schemas.enable` with default value `false`

These options will be automatically configured in case they are not present in the `config` object.

INFO:: The Cluster Operator does not validate the provided configuration. 
When invalid configuration is provided, the Kafka Connect cluster might not start or might become unstable. 
In such cases, the configuration in the `config` object should be fixed and the Cluster Operator will roll out the new configuration to all Kafka Connect instances.

===== Logging
The `logging` field allows the configuration of loggers. These loggers are:
[source]
log4j.rootLogger
connect.root.logger.level
log4j.logger.org.apache.zookeeper
log4j.logger.org.I0Itec.zkclient
log4j.logger.org.reflections

For information on the logging options and examples of how to set logging, see <<logging_examples, logging examples>> for Kafka.

===== Kafka Connect S2I deployment

When using {ProductName} together with an {OpenShiftName} cluster, a user can deploy Kafka Connect with support for https://docs.openshift.org/3.9/dev_guide/builds/index.html[{OpenShiftName} Builds] and https://docs.openshift.org/3.9/creating_images/s2i.html#creating-images-s2i[Source-to-Image (S2I)].
To activate the S2I deployment a `KafkaConnectS2I` resource should be used instead of a `KafkaConnect` resource.
The following is a full example of `KafkaConnectS2I` resource.

.Example `KafkaConnectS2I` resource
[source,yaml,options="nowrap",subs="attributes"]
----
apiVersion: {KafkaConnectS2I}
kind: KafkaConnectS2I
metadata:
  name: my-connect-cluster
spec:
  nodes: 1
  image: {DockerKafkaConnectS2I}
  readinessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  config: 
    bootstrap.servers: my-cluster-kafka:9092
----

The S2I deployment is very similar to the regular Kafka Connect deployment (as represented by the `KafkaConnect` resource).
Compared to the regular deployment, the Cluster Operator will create the following additional resources:

[connect-cluster-name]-connect-source::
ImageStream which is used as the base image for the newly-built Docker images.
[connect-cluster-name]-connect::
BuildConfig which is responsible for building the new Kafka Connect Docker images.
[connect-cluster-name]-connect::
ImageStream where the newly built Docker images will be pushed.
[connect-cluster-name]-connect::
DeploymentConfig which is in charge of creating the Kafka Connect worker node pods.
[connect-cluster-name]-connect::
Service which exposes the REST interface for managing the Kafka Connect cluster.

The Kafka Connect S2I deployment supports the same options as the regular Kafka Connect deployment.
A list of supported options can be found in the <<kafka_connect_config_map_details>> section.
The `image` option specifies the Docker image which will be used as the _source image_ - the base image for the newly built Docker image.
The default value of the `image` option is determined by the value of the `<<STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE,STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE>>` environment variable of the Cluster Operator.
All other options have the same meaning as for the regular `KafkaConnect` deployment.

Once the Kafka Connect S2I cluster is deployed, new plugins can be added by starting a new {OpenShiftName} build.
Before starting the build, a directory with all the KafkaConnect plugins which should be added has to be created.
The plugins and all their dependencies can be in a single directory or can be split into multiple subdirectories.
For example:

[source,shell]
----
$ tree ./s2i-plugins/
./s2i-plugins/
├── debezium-connector-mysql
│   ├── CHANGELOG.md
│   ├── CONTRIBUTE.md
│   ├── COPYRIGHT.txt
│   ├── debezium-connector-mysql-0.7.1.jar
│   ├── debezium-core-0.7.1.jar
│   ├── LICENSE.txt
│   ├── mysql-binlog-connector-java-0.13.0.jar
│   ├── mysql-connector-java-5.1.40.jar
│   ├── README.md
│   └── wkb-1.0.2.jar
└── debezium-connector-postgres
    ├── CHANGELOG.md
    ├── CONTRIBUTE.md
    ├── COPYRIGHT.txt
    ├── debezium-connector-postgres-0.7.1.jar
    ├── debezium-core-0.7.1.jar
    ├── LICENSE.txt
    ├── postgresql-42.0.0.jar
    ├── protobuf-java-2.6.1.jar
    └── README.md
----

A new build can be started using the following command:

[source,shell]
oc start-build my-connect-cluster-connect --from-dir ./s2i-plugins/

This command will upload the whole directory into the {OpenShiftName} cluster and start a new build.
The build will take the base Docker image from the source ImageStream (named _[connect-cluster-name]-connect-source_) and add the directory and all the files it contains into this image and push the resulting image into the target ImageStream (named _[connect-cluster-name]-connect_).
When the new image is pushed to the target ImageStream, a rolling update of the Kafka Connect S2I deployment will be started and will roll out the new version of the image with the added plugins.
By default, the `oc start-build` command will trigger the build and complete.
The progress of the build can be observed in the {OpenShiftName} console.
Alternatively, the option `--follow` can be used to follow the build from the command line:

[source,shell]
----
oc start-build my-connect-cluster-connect --from-dir ./s2i-plugins/ --follow
Uploading directory "s2i-plugins" as binary input for the build ...
build "my-connect-cluster-connect-3" started
Receiving source from STDIN as archive ...
Assembling plugins into custom plugin directory /tmp/kafka-plugins
Moving plugins to /tmp/kafka-plugins

Pushing image 172.30.1.1:5000/myproject/my-connect-cluster-connect:latest ...
Pushed 6/10 layers, 60% complete
Pushed 7/10 layers, 70% complete
Pushed 8/10 layers, 80% complete
Pushed 9/10 layers, 90% complete
Pushed 10/10 layers, 100% complete
Push successful
----

NOTE: The S2I build will always add the additional Kafka Connect plugins to the original source image.
They will not be added to the Docker image from a previous build.
To add multiple plugins to the deployment, they all have to be added within the same build.


[id="provisioning-rbac-for-the-cluster-operator"]

=== Provisioning Role-Based Access Control (RBAC) for the Cluster Operator

For the Cluster Operator to function it needs permission within the {ProductPlatformName} cluster to interact with the resources it manages and interacts with. 
These include the desired resources, such as  `Kafka`, `Kafka Connect`, and so on, as well as the managed resources, such as `ConfigMaps`, `Pods`, `Deployments`, `StatefulSets`, `Services`, and so on.
Such permission is described in terms of {ProductPlatformName} role-based access control (RBAC) resources:

* `ServiceAccount`,
* `Role` and `ClusterRole`,
* `RoleBinding` and `ClusterRoleBinding`.

In addition to running under its own `ServiceAccount` with a `ClusterRoleBinding`, the Cluster Operator manages some RBAC resources because some of the components need access to {ProductPlatformName} resources.

{ProductPlatformName} also includes privilege escalation protections that prevent components operating under one `ServiceAccount` from granting other `ServiceAccounts` privileges that the granting `ServiceAccount` does not have.
Because the Cluster Operator must be able to create the `ClusterRoleBindings` and `RoleBindings` needed by resources it manages, the Cluster Operator must also have those same privileges.

[id="delegated-privileges"]
==== Delegated privileges

When the Cluster Operator deploys resources for a desired `Kafka` resource it also creates `ServiceAccounts`, `RoleBindings` and `ClusterRoleBindings`, as follows:

* The Kafka broker pods use a `ServiceAccount` called `<cluster-name>-kafka` (where `<cluster-name>` is a placeholder for the name of the `Kafka` resource)
  - When the rack feature is used, the `strimzi-<cluster-name>-kafka-init` `ClusterRoleBinding` is used to grant this `ServiceAccount` access to the nodes within the cluster via a `ClusterRole` called `strimzi-kafka-broker`
  - When the rack feature is not used no binding is created.
* The Zookeeper pods use the default `ServiceAccount` as they have no need to access {ProductPlatformName} resources
* The Topic Operator pod uses a `ServiceAccount` called `<cluster-name>-topic-operator` (where `<cluster-name>` is a placeholder for the name of the `Kafka` resource)
    - The Topic Operator produces {ProductPlatformName} events with status information, so the `ServiceAccount` is bound to a `ClusterRole` called `strimzi-topic-operator` which grants this access via the `strimzi-topic-operator-role-binding` `RoleBinding`.

The pods for `KafkaConnect` and `KafkaConnectS2I` resources use the default `ServiceAccount`, since they require no access to {ProductPlatformName} resources.


==== Using a `ServiceAccount`

The Cluster Operator is best run using a `ServiceAccount`:

[source,yaml,options="nowrap"]
.Example `ServiceAccount` for the Cluster Operator
----
include::examples/install/cluster-operator/01-ServiceAccount-strimzi-cluster-operator.yaml[]
----

The `Deployment` of the operator then needs to specify this in its `spec.template.spec.serviceAccountName`:

[source,yaml,numbered,options="nowrap",highlight='12']
.Partial example `Deployment` for the Cluster Operator
----
include::examples/install/cluster-operator/05-Deployment-strimzi-cluster-operator.yaml[lines=1..13]
      # ...
----

Note line 12, where the the `strimzi-cluster-operator` `ServiceAccount` is specified as the `serviceAccountName`.


==== Defining `ClusterRoles`

The Cluster Operator needs to operate using `ClusterRoles` that give it access to the necessary resources. 
Depending on the {ProductPlatformName} cluster setup, a cluster administrator might be needed to create the `ClusterRoles`.

NOTE: Cluster administrator rights are only needed for the creation of the `ClusterRoles`. 
The Cluster Operator will not run under the cluster admin account.

The `ClusterRoles` follow the "principle of least privilege" and contain only those privileges needed by the Cluster Operator to operate Kafka, Kafka Connect, and Zookeeper clusters. The first set of assigned privileges allow the Cluster Operator to manage {ProductPlatformName} resources such as `StatefulSets`, `Deployments`, `Pods`, and `ConfigMaps`.

[source,yaml,options="nowrap"]
.Example `Role` for the Cluster Operator
----
include::examples/install/cluster-operator/02-ClusterRole-strimzi-cluster-operator-role.yaml[]
----

The `strimzi-kafka-broker` `ClusterRole` represents the access needed by the init container in Kafka pods that is used for the rack feature. As described in the xref:delegated-privileges[Delegated privileges] section, this role is also needed by the Cluster Operator in order to be able to delegate this access.

[source,yaml,options="nowrap"]
.`ClusterRole` for the Cluster Operator allowing it to delegate access to {ProductPlatformName} nodes to the Kafka broker pods
----
# These are the privileges needed by the init container
# in Kafka broker pods.
# The Cluster Operator also needs these privileges since 
# it binds the Kafka pods' ServiceAccount to this 
# role.
include::examples/install/cluster-operator/03-ClusterRole-strimzi-kafka-broker.yaml[]
----

The `strimzi-topic-operator` `ClusterRole` represents the access needed by the Topic Operator. As described in the xref:delegated-privileges[Delegated privileges] section, this role is also needed by the Cluster Operator in order to be able to delegate this access.

[source,yaml,options="nowrap"]
.`ClusterRole` for the Cluster Operator allowing it to delegate access to events to the Topic Operator
----
# These are the privileges needed by the Topic Operator.
# The Cluster Operator also needs these privileges since 
# it binds the Topic Operator's  ServiceAccount to this 
# role.
include::examples/install/cluster-operator/04-ClusterRole-strimzi-topic-operator.yaml[]
----


==== Defining `ClusterRoleBindings`

The operator needs a `ClusterRoleBinding` which associates its `ClusterRole` with its `ServiceAccount`:

[source,yaml,options="nowrap"]
.Example `RoleBinding` for the Cluster Operator
----
include::examples/install/cluster-operator/02-ClusterRoleBinding-strimzi-cluster-operator.yaml[]
----

`ClusterRoleBindings` are also needed for the `ClusterRoles` needed for delegation:

[source,yaml,options="nowrap"]
.Example `RoleBinding` for the Cluster Operator
----
---
include::examples/install/cluster-operator/03-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml[]
---
include::examples/install/cluster-operator/04-ClusterRoleBinding-strimzi-cluster-operator-topic-operator-delegation.yaml[]
---
----

=== Operator configuration

The operator itself can be configured through the following environment variables.

[[STRIMZI_NAMESPACE]] `STRIMZI_NAMESPACE`:: Required. A comma-separated list of namespaces that the operator should
operate in. The Cluster Operator deployment might use the https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api[{KubernetesName} Downward API]
to set this automatically to the namespace the Cluster Operator is deployed in. See the example below:
+
[source,yaml,options="nowrap"]
----
env:
  - name: STRIMZI_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
----

[[STRIMZI_FULL_RECONCILIATION_INTERVAL_MS]] `STRIMZI_FULL_RECONCILIATION_INTERVAL_MS`:: Optional, default: 120000 ms. The interval between periodic reconciliations, in milliseconds.


[[STRIMZI_OPERATION_TIMEOUT_MS]] `STRIMZI_OPERATION_TIMEOUT_MS`:: Optional, default: 300000 ms. The timeout for internal operations, in milliseconds. This value should be
increased when using {ProductName} on clusters where regular {ProductPlatformName} operations take longer than usual (because of slow downloading of Docker images, for example).

[[STRIMZI_DEFAULT_KAFKA_IMAGE]] `STRIMZI_DEFAULT_KAFKA_IMAGE`:: Optional, default `strimzi/kafka:latest`.
The image name to use as a default when deploying Kafka, if
no image is specified as the `kafka-image` in the <<kafka_config_map_details,Kafka cluster ConfigMap>>.

[[STRIMZI_DEFAULT_KAFKA_INIT_IMAGE]] `STRIMZI_DEFAULT_KAFKA_INIT_IMAGE`:: Optional, default `strimzi/kafka-init:latest`.
The image name to use as default for the init container started before the broker for doing initial configuration work (that is, rack support), if no image is specified as the `kafka-init-image` in the <<kafka_config_map_details,Kafka cluster ConfigMap>>.

[[STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE]] `STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE`:: Optional, default `strimzi/kafka-stunnel:latest`.
The image name to be used as a default when deploying the sidecar container which provides TLS support for Kafka if
no image is specified as the `spec.kafka.tlsSidecar.image` in the <<kafka_config_map_details,Kafka cluster ConfigMap>>.

[[STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE]] `STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE`:: Optional, default `strimzi/kafka-connect:latest`.
The image name to use as a default when deploying Kafka Connect, if
no image is specified as the `image` in the
<<kafka_connect_config_map_details,Kafka Connect cluster ConfigMap>>.

[[STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE]] `STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE`:: Optional, default `strimzi/kafka-connect-s2i:latest`.
The image name to use as a default when deploying Kafka Connect S2I, if
no image is specified as the `image` in the cluster ConfigMap.

[[STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE]] `STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE`:: Optional, default `strimzi/topic-operator:latest`.
The image name to use as a default when deploying the topic operator, if
no image is specified as the `image` in the <<topic_operator_json_config,topic operator config>>
of the Kafka cluster ConfigMap.

[[STRIMZI_DEFAULT_ZOOKEEPER_IMAGE]] `STRIMZI_DEFAULT_ZOOKEEPER_IMAGE`:: Optional, default `strimzi/zookeeper:latest`.
The image name to use as a default when deploying Zookeeper, if
no image is specified as the `zookeeper-image` in the <<kafka_config_map_details,Kafka cluster ConfigMap>>.

[[STRIMZI_DEFAULT_TLS_SIDECAR_ZOOKEEPER_IMAGE]] `STRIMZI_DEFAULT_TLS_SIDECAR_ZOOKEEPER_IMAGE`:: Optional, default `strimzi/zookeeper-stunnel:latest`.
The image name to use as a default when deploying the sidecar container which provides TLS support for Zookeeper, if
no image is specified as the `spec.zookeeper.tlsSidecar.image` in the <<kafka_config_map_details,Kafka cluster ConfigMap>>.

[[STRIMZI_LOG_LEVEL]] `STRIMZI_LOG_LEVEL`:: Optional, default `INFO`.
The level for printing logging messages. The value can be set to: `ERROR`, `WARNING`, `INFO`, `DEBUG` and `TRACE`.

[[STRIMZI_DEFAULT_TLS_SIDECAR_TOPIC_OPERATOR_IMAGE]] `STRIMZI_DEFAULT_TLS_SIDECAR_TOPIC_OPERATOR_IMAGE`:: Optional, default `strimzi/topic-operator-stunnel:latest`.
The image name to use as a default when deploying the sidecar container which provides TLS support for the Topic Operator, if
no image is specified as the `spec.topicOperator.tlsSidecar.image` in the <<kafka_config_map_details,Kafka cluster ConfigMap>>.

[[multi-namespace]]
==== Watching multiple namespaces

The `STRIMZI_NAMESPACE` environment variable can be used to configure a single operator instance
to operate in multiple namespaces. For each namespace given, the operator will watch for cluster ConfigMaps
and perform periodic reconciliation. To be able to do this, the operator's ServiceAccount needs
access to the necessary resources in those other namespaces. This can be done by creating an additional
RoleBinding in each of those namespaces, associating the operator's ServiceAccount
(`strimzi-cluster-operator` in the examples) with the operator's
Role (`strimzi-operator-role` in the examples).

Suppose, for example, that a operator deployed in namespace `foo` needs to operate in namespace `bar`.
The following RoleBinding would grant the necessary permissions:

.Example RoleBinding for a operator to operate in namespace `bar`
[source,yaml,options="nowrap"]
----
apiVersion: v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator-binding-bar
  namespace: bar
  labels:
    app: strimzi
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: foo
roleRef:
  kind: Role
  name: strimzi-cluster-operator-role
  apiGroup: v1
----
