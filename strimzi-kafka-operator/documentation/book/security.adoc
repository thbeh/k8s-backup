== Security

The Strimzi project supports data encryption of communication between the different Kafka and Strimzi components (Kafka brokers, Zookeeper nodes, Kafka Connect and Strimzi Topic Operator) by means of the SSL/TLS protocol.
This makes it possible to encrypt data transferred between Kafka brokers (interbroker communication), between Zookeeper nodes (internodal communication), and between clients and Kafka brokers.
For the Kafka and Strimzi components, TLS certificates are also used for authentication.

The Cluster Operator sets up the SSL/TLS certificates to provide this encryption and authentication.

=== Certificates

Each component needs its own private and public keys in order to support encryption.
The public key has to be signed by a certificate authority (CA) in order to have a related X.509 certificate for providing server authentication and encrypting the communication channel with the client (which could be another broker as well).
All component certificates are signed by a Certification Authority (CA) called _cluster CA_.
Another CA is used for authentication of Kafka clients connecting to Kafka brokers.
This CA is called _clients CA_.
The CAs themselves use self-signed certificates.

All of the generated certificates are saved as Secrets in the {ProductPlatformName} cluster, named as follows:

`<cluster-name>-cluster-ca`::
Contains the private and public keys of the cluster CA which is used for signing server certificates for the Kafka and Strimzi components (Kafka brokers, Zookeeper nodes, and so on).
`<cluster-name>-cluster-ca-cert`::
Contains only the public key of the cluster CA.
The public key used by Kafka clients to verify the identity of the Kafka brokers they are connecting to (TLS server authentication).
`<cluster-name>-clients-ca`::
Contains the private and public keys of the clients CA.
The clients CA is used for TLS client authentication of Kafka clients when connecting to Kafka brokers.
`<cluster-name>-clients-ca-cert`::
Contains only the public key of the client CA.
The public key is used by the Kafka brokers to verify the identity of Kafka clients when TLS client authentication is used.
`<cluster-name>-kafka-brokers`::
Contains all the Kafka broker private and public keys (certificates signed with the cluster CA).
`<cluster-name>-zookeeper-nodes`::
Contains all the Zookeeper node private and public keys (certificates signed with the cluster CA).
`<cluster-name>-topic-operator-certs`::
Contains the private and public keys (certificates signed with the cluster CA) used for encrypting communication between the Topic Operator and Kafka or Zookeeper.

All the keys are 2048 bits in size and are valid for 365 days from initial generation.

NOTE: "certificates rotation" for generating new ones on their expiration will be supported in future releases.

=== Interbroker Kafka Communication

Communication between brokers is done through the `REPLICATION` listener on port 9091, which is encrypted by default.

=== Zookeeper Communication

By deploying an `stunnel` sidecar within every Zookeeper pod, the Cluster Operator is able to provide data encryption and authentication between Zookeeper nodes in a cluster.
The `stunnel` sidecar proxies all Zookeeper traffic, TLS decrypting data upon entry into a Zookeeper pod and TLS encrypting data upon departure from a Zookeeper pod.
This TLS encrypting `stunnel` proxy is instantiated from the `spec.zookeeper.stunnelImage` specified in the Kafka resource.

=== Kafka Client connections via TLS

Encrypted communication between Kafka brokers and clients is provided through the `CLIENTTLS` listener on port 9093.

NOTE: You can use the `CLIENT` listener on port 9092 for unencrypted communication with brokers.

If a Kafka client wants to connect to the encrypted listener (`CLIENTTLS`) on port 9093, it needs to trust the cluster CA certificate in order to verify the broker certificate received during the SSL/TLS handshake.
The cluster CA certificate can be extracted from the generated `<cluster-name>-cluster-ca-cert` `Secret`.

ifdef::Kubernetes[]
On {KubernetesName}, the certificate can be extracted with the following command:

[source,shell]
kubectl get secret <cluster-name>-cluster-ca-cert -o jsonpath='{.data.ca\.crt}' | base64 -d > ca.crt

endif::Kubernetes[]

On {OpenShiftName}, the certificate can be extracted with the following command:

[source,shell]
oc get secret <cluster-name>-cluster-ca-cert -o jsonpath='{.data.ca\.crt}' | base64 -d > ca.crt

The Kafka client has to be configured to trust certificates signed by this CA.
For the Java-based Kafka Producer, Consumer and Streams APIs, you can do this by importing the CA certificate into the JVM's truststore using the following keytool command:

[source,shell]
keytool -keystore client.truststore.jks -alias CARoot -import -file ca.crt

Finally, in order to configure the Kafka client, following properties should be specified:

* `security.protocol`: SSL is the value for using encryption.
* `ssl.truststore.location`: the truststore location where the certificates were imported.
* `ssl.truststore.password`: password for accessing the truststore. This property can be omitted if it is not needed by the truststore.

The current implementation does not support Subject Alternative Names (SAN) so the hostname verification should be disabled on the client side.
For doing so the `ssl.endpoint.identification.algorithm` property needs to be set as empty.
