##################
# Configure Zeppelin, Jupyter, History server, Spark resource staging and Spark shuffle service here. 
# This umbrella chart configuration overrides the values.yaml in the sub-charts
##################

##################
# GLOBAL ATTRIBUTES FOR UMBRELLA CHART
##################
global:
  mountSecrets: true
  mountSparkConf: true
  mountZeppelinConf: true
  mountJupyterConf: true
  serviceAccount: default
  # internal attribute. Do not change
  umbrellaChart: true

##################
# CONFIGURATION FOR ZEPPELIN ENVIRONMENT
##################
zeppelin:
  # whether to enable Zeppelin
  enabled: true
  # Any environment variables that need to be made available to the container are defined here
  # This may include environment variables used by Spark, Zeppelin

  # sparkonk8s-test.json is name of the keyfile to access GCS bucket got history log.
  # Change it to reflect name of your key file. Key file will always be in path /etc/secrets
  environment:
    SPARK_SUBMIT_OPTIONS: >-
       --conf spark.kubernetes.driver.docker.image=snappydatainc/spark-driver:v2.2.0-kubernetes-0.5.1
       --conf spark.kubernetes.executor.docker.image=snappydatainc/spark-executor:v2.2.0-kubernetes-0.5.1
       --conf spark.executor.instances=2
#       --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/sparkonk8s-test.json
  sparkEventLog:
    enableHistoryEvents: false
    # eventsLogDir should point to a URI of GCS bucket where history events will be dumped
    eventLogDir: "gs://spark-history-server-store/"
  noteBookStorage:
    usePVForNoteBooks: true
    # If using PV for notebook storage, 'notebookDir' will be an
    # absolute path in the mounted persistent volume
    notebookDir: "/notebooks"

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: true
    ## If 'existingClaim' is defined, PVC must be created manually before
    ## volume will be bound
    # existingClaim:

    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, Azure & OpenStack)
    ##
    # storageClass: "-"
    storageClass: "spark-gluster"
    accessMode: ReadWriteOnce
    size: 8Gi
    # Whether to keep the PVC when chart is deleted, if PV is dynamically provisioned
    keepResource: false

  resources: {}
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi

##################
# CONFIGURATION FOR JUPYTER NOTEBOOK ENVIRONMENT
##################
jupyter:
  # Set false to exclude launching Jupyter notebook server with this Helm chart.
  enabled: false

  # If using PV for notebook storage, provide absolute path starting with /data/ in jupyter conf file
  # using 'c.NotebookApp.notebook_dir'. See jupyter_notebook_config.py.template

  sparkEventLog:
    enableHistoryEvents: false
    # eventsLogDir should point to a URI of GCS bucket where history events will be dumped
    eventLogDir: "gs://spark-history-server-store/"
    # Also, edit conf/spark/spark-defaults.conf to specify the keyfile for Google Cloud service account.

  persistence:
    enabled: true
    ## If 'existingClaim' is defined, PVC must be created manually before
    ## volume will be bound
    # existingClaim:

    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, Azure & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 6Gi
    # Whether to keep the PVC when chart is deleted, if PV is dynamically provisioned
    keepResource: false

  resources: {}
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi

##################
# CONFIGURATION FOR SPARK HISTORY SERVER
##################
historyserver:
  # whether to enable history server
  enabled: false
  historyServerConf:
    # URI of the GCS bucket
    # eventsDir: "gs://spark-history-server-store/"
    eventsDir: "s3a://thbeh/"

    # sparkonk8s-test.json is name of the keyfile to access GCS bucket got history log.
    # Change it to reflect name of your key file. Key file will always be in path /etc/secret
  environment:
    # SPARK_HISTORY_OPTS: -Dspark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/sparkonk8s-test.json
    SPARK_HISTORY_OPTS: -Dspark.hadoop.fs.s3a.endpoint=http://192.168.100.201:9000 -Dspark.hadoop.fs.s3a.access.key=minio -Dspark.hadoop.fs.s3a.secret.key=minio123 -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Dspark.hadoop.fs.s3a.path.style.access=true

  resources: {}
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi

##################
# configuration for Spark Resource Staging Server
##################
rss:
  # properties that can be made available via configmap
  properties:
      spark.ssl.kubernetes.resourceStagingServer.enabled: false
resources: {}
#  limits:
#    cpu: 100m
#    memory: 1Gi
#  requests:
#    cpu: 100m
#    memory: 256Mi

##################
# configuration for Spark Shuffle Service
##################
shuffle:
  shufflePodLabels:
    app: spark-shuffle-service
    spark-version: 2.2.0
  resources: {}
#    limits:
#      cpu: 200m
#    #  memory: 128Mi
#    requests:
#      cpu: 200m
#    #  memory: 128Mi
