# Umbrella Helm chart to deploy Spark on Kubernetes
This chart deploys Zeppelin, Jupyter, Spark History Server, Spark Resource Staging Server, 
and Spark Shuffle Service on Kubernetes. This chart uses individual subcharts for 
the respective components. A user can configure each subchart in parent chart's 
`values.yaml` file.

# Usage
#### Launch Spark and Notebook servers

We use the spark-umbrella chart to deploy Jupyter, Zeppelin, Spark Resource Staging Server, and Spark Shuffle Service 
on Kubernetes. This chart is composed from individual sub-charts for each of the components. 

You can configure the components in the umbrella chart's 'values.yaml' (see spark-umbrella/values.yaml) or in 
each of the individual subchart's 'values.yaml' file. The umbrella chart's 'values.yaml' will override the component one.

```text
# fetch the chart repo ....
git clone https://github.com/SnappyDataInc/spark-on-k8s

# Get the subcharts required by the umbrella chart
cd charts
helm dep up spark-umbrella

# Now, install the chart
helm install --name spark-all ./spark-umbrella/
```

> Note that this command will return quickly and kubernetes controllers will work in the background to achieve the state
specified in the chart. The command below can be used to access the notebook environment from any browser. 

```text
kubectl get services -w
# Note: this could take a while to complete. Use '-w' option to wait for state changes. 
```

Once everything is up and running you will see something like this:
```text
NAME                            TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                         AGE
kubernetes                      ClusterIP      10.63.240.1     <none>          443/TCP                         1d
spark-all-jupyter-spark         LoadBalancer   10.63.246.130   35.184.71.164   8888:31540/TCP,4040:30922/TCP   9m
spark-all-rss                   LoadBalancer   10.63.246.190   35.192.235.35   10000:31000/TCP                 9m
spark-all-zeppelin              LoadBalancer   10.63.254.150   35.192.68.147   8080:30522/TCP,4040:31236/TCP   9m
```
> Access the zeppelin notebook environment using URL external-ip:8080 from any browser.
> Spark UI is accessible using URL external-ip:4040. 
NOTE that the Spark UI is only accessible after you have run at least one Spark job. Spark Driver (and hence UI) is lazily started. 
Simply navigate to <Zeppelin Home>; Click 'Zeppelin Tutorial' and then 'Basic Features(Spark)'. Run the 'Load
data' paragraph followed by one or more SQL paragraphs. Similarly Spark UI for for jobs run using Jupyer notebook will be available
once you run the job.

#### Configuring History server
In this example, we use Google Cloud Storage(GCS) to persist the events generated by Spark applications. You don't need 
the steps below if you decide to use other schemes like 'hdfs' or 's3' storage.

Using Google cloud utilities (gsutil and gcloud ; should already be setup on your local laptop), we create a GCS bucket 
and associate it with your GCP project.  

    ```
    # Create a bucket using gsutil
    # NOTE: Bucket names have to be globally unique. Pick a unique name if spark-history-server bucket exists.
    gsutil mb -c nearline gs://spark-history-server-store
    # Specify a account name
    export ACCOUNT_NAME=sparkonk8s-test
    # Change below to specify your Google cloud project name. Use 'gcloud config list' if you don't know. 
    export GCP_PROJECT_ID= your-gcp-project-id
    # Create a service account and generate credentials
    gcloud iam service-accounts create ${ACCOUNT_NAME} --display-name "${ACCOUNT_NAME}"
    gcloud iam service-accounts keys create "${ACCOUNT_NAME}.json" --iam-account "${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com"
    # Grant admin rights to the bucket
    gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} --member "serviceAccount:${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" --role roles/storage.admin
    gsutil iam ch serviceAccount:${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com:objectAdmin gs://spark-history-server-store
    ```
In order for history server to be able read from the GCS bucket, we need to mount the json key file on the history 
server pod. Copy the json file into 'conf/secrets' directory for umbrella chart.

```text
cp sparkonk8s-test.json spark-umbrella/conf/secrets/
```

By default, umbrella chart does not deploy the History server. We enable the History server deployment by modifying the
'values.yaml' file (in the zeppelin-with-spark folder). We also specify the GCS bucket path created above. 
History server will read spark events from this path.  

```text
historyserver:
  # whether to enable history server
  enabled: true
  historyServerConf:
    # URI of the GCS bucket
    eventsDir: "gs://spark-history-server-store"
```

Next, set the SPARK_HISTORY_OPTS so that history server uses json key file while accessing the GCS bucket
```text
environment:
  SPARK_HISTORY_OPTS: -Dspark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/sparkonk8s-test.json
```

Finally, we configure Zeppelin to log events to the same GCS bucket

```text
zeppelin:

  environment:
    SPARK_SUBMIT_OPTIONS: >-
       --kubernetes-namespace default
       --conf spark.kubernetes.driver.docker.image=snappydatainc/spark-driver:v2.2.0-kubernetes-0.5.1
       --conf spark.kubernetes.executor.docker.image=snappydatainc/spark-executor:v2.2.0-kubernetes-0.5.1
       --conf spark.executor.instances=2
       --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/sparkonk8s-test.json

  sparkEventLog:
    enableHistoryEvents: true
    # eventsLogDir should point to a URI of GCS bucket where history events will be dumped
    eventLogDir: "gs://spark-history-server-store"
```

#### Launch Spark, Zeppelin and History server cluster

Follow the Helm install command to launch everything. For Spark batch job (Spark-submit) follow instructions 
below (you need additional configuration)

```
helm install --name spark-all ./spark-umbrella/
```

You can access the History server UI using URL History-server-external-IP:18080
> Note: When using GCS for logging the logs become visible only when the Spark application exits. You may have to 
restart the Zeppelin interpreter to view the logs. Use the Zeppelin Spark Driver UI for current state.   
> The spark-submit logs should be immediately accessible from the history server

##### Enable spark-submit to log spark history events
The spark-submit example below shows Spark job that logs historical events to the GCS bucket created in above steps. 
Once job finishes, use the Spark history server UI to view the job execution details.

  ```
  bin/spark-submit \
      --master k8s://https://<k8s-master-IP> \
      --deploy-mode cluster \
      --name spark-pi \
      --class org.apache.spark.examples.SparkPi \
      --conf spark.eventLog.enabled=true \
      --conf spark.eventLog.dir=gs://spark-history-server-store/ \
      --conf spark.executor.instances=2 \
      --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/sparkonk8s-test.json \
      --conf spark.kubernetes.driver.secrets.history-secrets=/etc/secrets \
      --conf spark.kubernetes.executor.secrets.history-secrets=/etc/secrets \
      --conf spark.kubernetes.driver.docker.image=snappydatainc/spark-driver:v2.2.0-kubernetes-0.5.1 \
      --conf spark.kubernetes.executor.docker.image=snappydatainc/spark-executor:v2.2.0-kubernetes-0.5.1 \
      local:///opt/spark/examples/jars/spark-examples_2.11-2.2.0-k8s-0.5.0.jar
  ```

#### Deleting the chart
Use `helm delete` command to delete the chart
```text
helm delete --purge spark-all
```

